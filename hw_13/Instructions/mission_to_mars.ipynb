{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies to scrape\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/chromedriver\r\n"
     ]
    }
   ],
   "source": [
    "# Locate chromedriver\n",
    "! which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a path for browser function\n",
    "executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "browser = Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a url as a string and visit the site with browser\n",
    "url = \"https://mars.nasa.gov/news/\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab .html link and parse info\n",
    "nasa_news_html = browser.html\n",
    "soup = bs(nasa_news_html, \"html.parser\")\n",
    "# Scrape desired content and store\n",
    "news_title = soup.find(\"div\", class_=\"content_title\").text\n",
    "news_p = soup.find(\"div\", class_=\"article_teaser_body\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store next website's url as string and visit with the browser function\n",
    "url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store the .html link and parse info\n",
    "jpl_html = browser.html\n",
    "soup = bs(jpl_html, \"html.parser\")\n",
    "# Grab base_url and endpoint\n",
    "base_url = \"https://www.jpl.nasa.gov/\"\n",
    "endpoint_url = soup.find(\"article\", class_=\"carousel_item\")[\"style\"].split()[1].\\\n",
    "strip(\"\"\"\n",
    "url() \"\" '' ;\n",
    "\"\"\")\n",
    "# Concatenate strings and visit the url\n",
    "url = (base_url + endpoint_url)\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the .html link of the page and parse the info\n",
    "img_html = browser.html\n",
    "soup = bs(img_html, \"html.parser\")\n",
    "# Store the image url accordingly\n",
    "featured_image_url = soup.find(\"img\")[\"src\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the url of the next website as a string and visit the url\n",
    "url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the .html link and parse\n",
    "twit_html = browser.html\n",
    "soup = bs(twit_html, \"html.parser\")\n",
    "# Store the latest tweet\n",
    "mars_weather = soup.find(\"p\", class_=\"TweetTextSize\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the next url as a string and use pandas to read the table data\n",
    "url = \"https://space-facts.com/mars/\"\n",
    "table = pd.read_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the next url as a string and visit the website\n",
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the .html link and parse the info\n",
    "usgs_html = browser.html\n",
    "soup = bs(usgs_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results\n",
    "results = soup.find_all(\"div\", class_=\"description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every record, grab link\n",
    "taglink_list = []\n",
    "for result in results:\n",
    "    taglink_list.append(result.a[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store base url as string\n",
    "base_url = \"https://astrogeology.usgs.gov\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every page, grab the title and img endpoint url\n",
    "title_list = []\n",
    "endpoint_url_list = []\n",
    "for i in taglink_list:\n",
    "    url = (base_url + i)\n",
    "    browser.visit(url)\n",
    "    link_html = browser.html\n",
    "    # Grab title\n",
    "    soup = bs(link_html, \"html.parser\")\n",
    "    title_list.append(soup.find(\"h2\", class_=\"title\").text)\n",
    "    # Grab img endpoint url\n",
    "    endpoint_url_list.append(soup.find(\"img\", class_=\"wide-image\")[\"src\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every endpoint url, concatenate with base url\n",
    "full_url_list = []\n",
    "for i in endpoint_url_list:\n",
    "    full_url_list.append(base_url + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace unnecessary words accordingly\n",
    "title_list = [i.replace(\" Enhanced\", \"\").strip() for i in title_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the titles and urls of each instance as a dictionary\n",
    "hemi_list = []\n",
    "for i in title_list:\n",
    "    entry = {\"title\": i, \"img_url\": full_url_list[title_list.index(i)]}\n",
    "    hemi_list.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all scraped data into one dictionary\n",
    "scraped_data = {\"NASA Mars News Article\": news_title,\\\n",
    "                \"Short Summary\": news_p,\\\n",
    "                \"JPL Featured Space Image URL\": featured_image_url,\\\n",
    "                \"Mars Weather Twitter Account (Latest Tweet)\": mars_weather,\\\n",
    "                \"Table of Facts about Mars\": table,\\\n",
    "                \"List of Mars Hemispheres and their Image URLs\": hemi_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script mission_to_mars.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
